{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45438b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "400c11e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/cleaned_train_part_0001.snappy.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c652d",
   "metadata": {},
   "source": [
    "## Векторизация текста\n",
    "\n",
    "Этап 1: TF-IDF для заголовков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e5f33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "all_titles = pd.concat([df['base_title'], df['cand_title']]).reset_index(drop=True)\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=50_000,\n",
    "    ngram_range=(1, 2),\n",
    "    tokenizer=lambda x: x.split(),\n",
    "    lowercase=False\n",
    ")\n",
    "title_vectors = tfidf.fit_transform(all_titles)\n",
    "\n",
    "base_title_vec = title_vectors[:len(df)]\n",
    "cand_title_vec = title_vectors[len(df):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f5c95",
   "metadata": {},
   "source": [
    "Этап 2: Word2Vec для описаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ebea16",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m all_descriptions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_description\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcand_description\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m      2\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m all_descriptions]\n\u001b[1;32m----> 4\u001b[0m w2v_model \u001b[38;5;241m=\u001b[39m Word2Vec(\n\u001b[0;32m      5\u001b[0m     sentences,\n\u001b[0;32m      6\u001b[0m     vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,    \u001b[38;5;66;03m# Размер вектора\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,          \u001b[38;5;66;03m# Размер контекста\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,       \u001b[38;5;66;03m# Игнорировать редкие слова\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,         \u001b[38;5;66;03m# Параллельные процессы\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m          \u001b[38;5;66;03m# Количество эпох обучения\u001b[39;00m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_to_vec\u001b[39m(text):\n\u001b[0;32m     14\u001b[0m     words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()  \u001b[38;5;66;03m# Разбиваем уже очищенную строку\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:429\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, total_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, start_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha, compute_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:491\u001b[0m, in \u001b[0;36mWord2Vec.build_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 491\u001b[0m total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_vocab(\n\u001b[0;32m    492\u001b[0m     corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, progress_per\u001b[38;5;241m=\u001b[39mprogress_per, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count \u001b[38;5;241m=\u001b[39m corpus_count\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words \u001b[38;5;241m=\u001b[39m total_words\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:586\u001b[0m, in \u001b[0;36mWord2Vec.scan_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_file:\n\u001b[0;32m    584\u001b[0m     corpus_iterable \u001b[38;5;241m=\u001b[39m LineSentence(corpus_file)\n\u001b[1;32m--> 586\u001b[0m total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scan_vocab(corpus_iterable, progress_per, trim_rule)\n\u001b[0;32m    588\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollected \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m word types from a corpus of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m raw words and \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_vocab), total_words, corpus_count\n\u001b[0;32m    591\u001b[0m )\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:570\u001b[0m, in \u001b[0;36mWord2Vec._scan_vocab\u001b[1;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m    565\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    566\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: at sentence #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m, processed \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m words, keeping \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m word types\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    567\u001b[0m         sentence_no, total_words, \u001b[38;5;28mlen\u001b[39m(vocab),\n\u001b[0;32m    568\u001b[0m     )\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence:\n\u001b[1;32m--> 570\u001b[0m     vocab[word] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    571\u001b[0m total_words \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentence)\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_vocab_size \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vocab) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_vocab_size:\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_descriptions = pd.concat([df['base_description'], df['cand_description']])\n",
    "sentences = [text.split() for text in all_descriptions]\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,    # Размер вектора\n",
    "    window=5,          # Размер контекста\n",
    "    min_count=3,       # Игнорировать редкие слова\n",
    "    workers=4,         # Параллельные процессы\n",
    "    epochs=10          # Количество эпох обучения\n",
    ")\n",
    "\n",
    "def text_to_vec(text):\n",
    "    words = text.split()  # Разбиваем уже очищенную строку\n",
    "    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_model.vector_size)\n",
    "\n",
    "base_desc_vec = np.array([text_to_vec(text) for text in df['base_description']])\n",
    "cand_desc_vec = np.array([text_to_vec(text) for text in df['cand_description']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856f351",
   "metadata": {},
   "source": [
    "Этап 3: Расчёт схожести заголовков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a48d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_similarity = cosine_similarity(base_title_vec, cand_title_vec)\n",
    "\n",
    "# Для каждой пары берём только соответствующую диагональ (base[i] vs cand[i])\n",
    "df['title_similarity'] = title_similarity.diagonal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550414d",
   "metadata": {},
   "source": [
    "Этап 4: Расчёт схожести описаний "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ade1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем списки векторов в numpy массивы\n",
    "base_desc_matrix = np.vstack(base_desc_vec)\n",
    "cand_desc_matrix = np.vstack(cand_desc_vec)\n",
    "\n",
    "description_similarity = cosine_similarity(base_desc_matrix, cand_desc_matrix)\n",
    "df['description_similarity'] = description_similarity.diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d799140",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['title_similarity'], bins=50, color='skyblue')\n",
    "plt.title('Распределение схожести заголовков')\n",
    "plt.xlabel('Косинусная схожесть')\n",
    "plt.ylabel('Количество пар')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['description_similarity'], bins=50, color='salmon')\n",
    "plt.title('Распределение схожести описаний')\n",
    "plt.xlabel('Косинусная схожесть')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
